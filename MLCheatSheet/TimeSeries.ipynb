{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis ¯\\\\_(ツ)_/¯\n",
    "- Python package: `statsmodels`\n",
    "- References: \n",
    "    - https://www.math.u-psud.fr/~goude/Materials/time_series/cours6_ARIMA.pdf\n",
    "    - http://www.unige.ch/ses/sococ/eda/bernard/box.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Generalities\n",
    "\n",
    "#### Stationary Time Series\n",
    "\n",
    "- Expected value does not depend on time.\n",
    "- Covariance($X_t$,$X_{t+k}$) depends only on $k$.  \n",
    "\n",
    "#### Autocovariance Function: \n",
    "\n",
    "For a time series $X$ of size $n$, $$ACovF(k)=\\frac{1}{n}\\sum_{j=1}^{n-k}(X_{j+k}-\\mu)(X_j-\\mu)$$\n",
    "$$\\mu = \\frac{1}{n}\\sum_{t=1}^n X_t$$\n",
    "\n",
    "#### Autocorrelation Function: $$ACorrF(k)=\\frac{ACovF(k)}{ACov(0)} = \\frac{ACovF(k)}{Cov(X)}$$\n",
    "\n",
    "#### Partial Autocorrelation Function:  $$PACorrF(k)= Corr\\left(X_{t+k}-P_{t,k}(X_{t+k}),X_{t}-P_{t,k}(X_{t})\\right) $$  \n",
    "with $P_{t,k}(X_t)$ the orthogonal projection of $X_t$ onto the linear subspace of Hilbert space spanned by $ x_{t+1},\\dots ,x_{t+k}$. \n",
    "> PACF not straightforward to compute, there are algorithms for estimating the partial autocorrelation based on the sample autocorrelations (Box, Jenkins, and Reinsel 2008 and Brockwell and Davis, 2009). These algorithms derive from the exact theoretical relation between the partial autocorrelation function and the autocorrelation function. - [source](https://en.wikipedia.org/wiki/Partial_autocorrelation_function)\n",
    "\n",
    "## 1. ARMA: Autoregressive Moving Average\n",
    "### About\n",
    "\n",
    "- Assumes time series is a stationary stochastic process\n",
    "- ARMA(p,q): $$X_t = \\gamma+ \\sum_{i=1}^{p}a_iX_{t-i} + \\sum_{j=0}^{q} b_i\\epsilon_{t-i}$$\n",
    "\n",
    "With $\\epsilon$ being white noise terms, $\\delta$ a drift term.\n",
    "\n",
    "### Fitting ARMA: The **Box-Jenkins** approach:\n",
    "\n",
    "\n",
    "##### 1. Plot ACF and PACF functions\n",
    "##### 2. Identify case:\n",
    "- **Exponential decay of ACF to 0**:\n",
    "    - Indicates an Autoregressive model (AR(p))\n",
    "    - Find the order $p$ (value at which PACF is in the **critical region**, a.k.a. statistially 0.  \n",
    "    \n",
    "- **One or more spikes, rest  being 0**:\n",
    "    - Indicates a Moving average model (MA(q))\n",
    "    - Find the order $q$ (value at which PACF is in the **critical region**, a.k.a. statistially 0.  \n",
    "\n",
    "- **Exponential decay starting after a few lags**:\n",
    "    - Indicates a Mixed autoregressive and moving average model.\n",
    "\n",
    ">**The partial autocorrelation of an AR(p) process is zero at lag $p + 1$ and greater**. If the sample autocorrelation plot indicates that an AR model may be appropriate, then the sample partial autocorrelation plot is examined to help identify the order. **One looks for the point on the PACF plot where the partial autocorrelations for all higher lags are essentially zero**. Placing on the plot an indication of the sampling uncertainty of the sample PACF is helpful for this purpose: this is usually constructed on the basis that the true value of the PACF, at any given positive lag, is zero. This can be formalised as described below.   \n",
    "\n",
    ">An **approximate test that a given partial correlation is zero (at a 5% significance level)** is given by comparing the sample partial autocorrelations against the **critical region with upper and lower limits given by $\\frac{±1.96}{\\sqrt(n)}$**, where $n$ is the record length (number of points) of the time-series being analysed. This approximation relies on the assumption that the record length is at least moderately large (say $n$>30) and that the underlying process has finite second moment.  - [source](https://en.wikipedia.org/wiki/Partial_autocorrelation_function)\n",
    "\n",
    "##### 3. Choose parameters which minify a penalty criterion:\n",
    "**Akaike's Information Criterion (AIC)**:\n",
    "$$AIC(p,q)=\\frac{1}{T}\\sum_{t=1}^{T} $$\n",
    " \n",
    "\n",
    "### Limits of ARMA\n",
    "\n",
    "- If ACF has no decay to zero or very slow decay, might be because of a trend (i.e. the time series is not stationary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ARIMA: Address non-stationary time series with polynomial trend\n",
    "### About\n",
    "- Improvement of ARMA when there is a **trend** in the process, making it **non-stationary**:\n",
    "$$X_t = \\sum_{i=1}^{p}a_iX_{t-i} + \\sum_{j=0}^{q} b_i\\epsilon_{t-i} + \\sum_{j=0}^{k} \\gamma_t t^k$$ \n",
    "- The \"I\" stands for *integrated*, and this is what's leveraged to manage the trend.\n",
    "- We build the differentiation operator $$\\Delta X_t = X_t - X_{t-1}$$\n",
    "- We build the k-th order differentiation operator $$\\Delta^kX_t = \\Delta\\left(\\Delta^{k-1}X_t\\right)$$\n",
    "\n",
    "- ARIMA(p,k,q): $$\\Delta^kX_t = \\gamma+   \\sum_{i=1}^{p}a_i\\Delta^kX_{t-i} + \\sum_{j=0}^{q} b_i\\epsilon_{t-i}$$\n",
    "\n",
    ">⚠ By differentiating the stochastic process $X$, we hope that the resulting process will be stationary and thus be approached by ARMA. Indeed, if $X$ presents a polynomial trend of degree $m$, $\\Delta^kX$ will present a trend of degree $m-k$\n",
    "\n",
    "\n",
    "### Theory\n",
    "We can rewrite:\n",
    "$$ \\left(\\Delta^kX_t-\\sum_{i=1}^{p}a_i\\Delta^kX_{t-i}\\right) = \\gamma+ \\sum_{j=0}^{q} b_i\\epsilon_{t-i}$$ \n",
    "\n",
    "By introducing the **lag operator $L$**:\n",
    "\n",
    "$$ \\left(1-\\sum_{i=1}^{p}a_iL^i\\right)\\Delta^kX_t = \\gamma+ \\sum_{j=0}^{q} b_i\\epsilon_{t-i}$$ \n",
    "\n",
    "Let's assume $\\left(1-\\sum_{i=1}^{p}a_iL^i\\right)$ has a unit root of multiplicity $k$:\n",
    "$$\\Leftrightarrow \\Phi(L)\\left(1-L\\right)^k X_t =    \\Theta(L)\\epsilon_t$$\n",
    "\n",
    "It comes:\n",
    "\n",
    ">$X$ has an ARIMA(p,k,q) representation \n",
    ">$$\\Leftrightarrow \n",
    "\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\textrm{There exist } \\Phi \\textrm{ and } \\Theta \\textrm{ so that:} \\\\\n",
    "                  \\Phi(L)\\left(1-L\\right)^k X_t =    \\Theta(L)\\epsilon_t\\\\\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "              \n",
    "### Fitting ARIMA\n",
    "&rarr; Again with **Akkaike's Information Criterion (AIC)**, but on the differentied process.\n",
    "   \n",
    "### Limits of ARIMA\n",
    "\n",
    "- If the ACF has **periodical lag spikes**, the process might include seasonality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accounting for polynomial trends & seasonality: SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About\n",
    "- We build the differentiation operator $$\\Delta_1 X_t = X_t - X_{t-1}$$\n",
    "- We build the k-th order differentiation operator $$\\Delta_kX_t = X_t - X_{t-k}$$\n",
    "\n",
    "\n",
    "### Theory\n",
    "- We follow the same reasoning as in section 2, which yields:\n",
    "\n",
    ">X has a SARIMA$\\left((p,k,q);(P,K,Q)\\right)$ representation \n",
    ">\n",
    ">$$\\Leftrightarrow \n",
    "\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  \\textrm{There exist } \\Phi_p, \\Phi_P, \\Theta_q, \\Theta_Q \\textrm{ so that:} \\\\\n",
    "                  \\Phi_p(L)\\left(1-L\\right)^k\\left(1-L^s\\right)^K\\Phi_P(L^s) X_t =    \\Theta_q(L)\\Theta_Q(L^s)\\epsilon_t\\\\\n",
    "                \\end{array}\n",
    "              \\right.$$\n",
    "              \n",
    "### Fitting SARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. RNNs\n",
    "\n",
    "Networks with an internal state $h$ (memory), adapted to the study of temporal processes. - [source](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "\n",
    "![Alt text](https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn.png)\n",
    "\n",
    "\n",
    "On an NLP task:  \n",
    "\n",
    "$x^{(t)}$ Batch of characters  \n",
    "$a^{(t)}$ activation of step t   \n",
    "$y^{(t)}$ output of layer t (Prediction of upcoming characters)\n",
    "\n",
    "\n",
    "|Pros | Cons |\n",
    "|---|--- |\n",
    "|$W$ are shared across time steps | Aggregating gradient on long sequences is expensive |\n",
    "|| Vanishing gradients|\n",
    "\n",
    "\n",
    "**Forward Pass**\n",
    "$$ y^{(t)} = g_2(W_{ya} a^{(t)})$$\n",
    "$$ a^{(t+1)} = g_1(W_{aa} a^{(t)} + W_{ax} x^{(t)})$$\n",
    "\n",
    "**Loss** \n",
    "\n",
    "\n",
    "With $y_{k}^{true}(\\tau)$ the target value of dimension $d(\\tau)$ at step $\\tau$, considering a quadratic loss:\n",
    "    $$ L(\\tau) = \\frac{1}{2}\\sum_{k=1}^{d(\\tau)} \\left(y_{k}^{true}(\\tau) - y_{k}(\\tau)\\right)^{2} $$\n",
    "\n",
    "\n",
    "The overall network error over time period $]t', t]$  becomes:\n",
    "    $$ L^{total}(t',t) = \\sum_{\\tau=t'+1}^{t} L(\\tau) $$\n",
    "\n",
    "**Gradient**\n",
    "    $$ \\nabla L^{total}(t',t) = \\sum_{\\tau=t'+1}^{t} \\nabla L(\\tau) $$\n",
    "\n",
    "**Parameters Update** - [source](https://stats.stackexchange.com/questions/219914/rnns-when-to-apply-bptt-and-or-update-weights)  \n",
    "\n",
    "$$ \\Delta W = - \\eta \\nabla_W L^{total}(t',t) $$\n",
    "\n",
    "> The efficient computation of gradient & parameter update is an active research field and sees various methods:\n",
    "\n",
    "### Using Real-Time Recurrent Learning\n",
    "TODO\n",
    "\n",
    "### Using  Backpropagation Through Time (BTT) on an input sequence defined between $[t_0, t_e]$:\n",
    "\n",
    "\n",
    "\n",
    "-> To be used if the stream is segmented into sequences independent of each other\n",
    "\n",
    "**Forward Pass** - Let the network run from step $t_0$ to $t_e$, save entire history of inputs, targets, and network state.\n",
    "\n",
    "**Single Backward Pass** - Compute $\\forall \\tau \\in [t_0,t_e], \\forall k \\in [0,d(\\tau)] $\n",
    "\n",
    "\n",
    "\n",
    "With **U the set of indices $l$ such that $w_{lk} \\in W_{aa}$**\n",
    "\\begin{equation}\n",
    "  \\delta_k(\\tau)=- \\frac{\\delta L^{total}(t_0,t_e)}{\\delta s_k(\\tau)} =\\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    f_k'(s_k(\\tau))e_k(\\tau), & \\text{if}\\ \\tau=t_e \\\\\n",
    "    f_k'(s_k(\\tau))\\left( \\mathbf{e_k(\\tau)} + \\sum_{l \\in U} w_{lk} \\delta_l (\\tau+1) \\right), & \\text{if}\\ t_0 \\leq \\tau \\leq t_e\n",
    "  \\end{array}\\right.\n",
    "\\end{equation} \n",
    "with $s_k(\\tau)$ the pre-activation of neuron $k$ at step $\\tau$\n",
    "\n",
    "Then $$\\Delta w_{ij} = - \\eta \\sum_{\\tau=t_0+1}^{t_e} \\delta_i(\\tau) x_j(\\tau-1)$$\n",
    "\n",
    ">This can be viewed as applying canonical backpropagation computation to a feedforward neural network **in which target values are specified in several layers and not just the last one** ! The bolded $e_k(\\tau)$  aswell as the sum over the various time steps in the loss are the only RNN-specific components. \n",
    "\n",
    "### Using Truncated Backpropagation Through Time (TBTT):\n",
    "\n",
    "Introduce an update frequency k1, and an aggregation window k2 during gradient computation.\n",
    "\n",
    "1. **Forward pass** - Step through the next k1 time steps, computing the input, hidden, and output states.\n",
    "2. **Compute the loss**, i.e. sum only over the k1 time steps.\n",
    "3. **Backward pass** - Accumulate over the previous k2 time steps (this requires having stored all activations for these time steps).\n",
    "4. **Update parameters** (this occurs once per chunk of size k1, not incrementally at each time step).\n",
    "5. If processing multiple chunks of a longer sequence, **store the hidden state at the last time step** (will be used to initialize hidden state for beginning of next chunk). If we've reached the end of the sequence, reset the memory/hidden state and move to the beginning of the next sequence (or beginning of the same sequence, if there's only one).\n",
    "6. Rinse & **repeat from step 1**.\n",
    "\n",
    "Gradient computation and updates are performed every k1 time steps because it's computationally cheaper than updating at every time step. Updating multiple times per sequence (i.e. setting k1\n",
    "\n",
    "less than the sequence length) can accelerate training because weight updates are more frequent.\n",
    "\n",
    "Backpropagation is performed for only k2\n",
    "time steps because it's computationally cheaper than propagating back to the beginning of the sequence (which would require storing and repeatedly processing all time steps). Gradients computed in this manner are an approximation to the 'true' gradient computed over all time steps. But, because of the vanishing gradient problem, gradients will tend to approach zero after some number of time steps; propagating beyond this limit wouldn't give any benefit. Setting k2 too short can limit the temporal scale over which the network can learn. However, the network's memory isn't limited to k2 time steps because the hidden units can store information beyond this period (e.g. see Mikolov 2012 and this post)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short-Term Memory Networks (LSTMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.LSTMs\n",
    "\n",
    "Developed because of the **vanishing/exploding gradient problem** in RNNs:   \n",
    "Indeed, while backpropagating the errors accross timesteps, we multiply at each timestep our local errors by $w_{lk}$, which makes $\\delta_k(\\tau) \\rightarrow 0/\\infty$ for $\\tau \\rightarrow t_0$\n",
    "\n",
    "![LSTM principle](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3b/The_LSTM_cell.png/1600px-The_LSTM_cell.png)\n",
    "\n",
    "C: state  \n",
    "First: decides what to forget  \n",
    "Second: decide what to add and in which qty  \n",
    "Third: decide what to output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Unobserved Components Models\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hierarchical time series:¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "combi",
   "language": "python",
   "name": "combi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
